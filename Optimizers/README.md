## Optimizers
Focus on papers and related resources that propose deep learning optimization algorithms.

## Papers
* (Adam)  **Adam: A Method for Stochastic Optimization**, *Diederik P. Kingma, Jimmy Ba*, ICLR 2015  [[PDF]](https://arxiv.org/pdf/1412.6980v9.pdf)  [[Official PyTorch Implementation]](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam)  #  [[Notes]](https://www.hjhgjghhg.com/index.php/archives/151/)
* (AMSGrad) **On the Convergence of Adam and Beyond**, *Sashank J. Reddi, Satyen Kale, Sanjiv Kumar*, ICLR 2018 (Best Paper)  [[ICLR PDF]](https://arxiv.org/)  [[Official PyTorch Implementation]](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam)  #  [[Notes]](https://www.hjhgjghhg.com/index.php/archives/158/)
* (SWATS) **Improving Generalization Performance by Switching from Adam to SGD**, *Nitish Shirish Keskar, Richard Socher*，arXiv 2017  [[PDF]](https://arxiv.org/pdf/1712.07628.pdf)  [[PyTorch Implementation]](https://github.com/Mrpatekful/swats)  #  [[Notes]](https://www.hjhgjghhg.com/index.php/archives/166/)
* (AdaBound) **Adaptive Gradient Methods with Dynamic Bound of Learning Rate**, *Liangchen Luo, Yuanhao Xiong, Yan Liu, Xu Sun*, ICLR 2019  [[ICLR PDF]](https://openreview.net/pdf?id=Bkg3g2R9FX)  [[Official PyTorch Implementation]](https://github.com/Luolc/AdaBound)  #  [[Notes]](https://www.hjhgjghhg.com/index.php/archives/166/)
* (AdamW) **Decoupled Weight Decay Regularization**，*Ilya Loshchilov, Frank Hutter*，ICLR 2019  [[ICLR PDF]](https://openreview.net/pdf?id=Bkg6RiCqY7)  [[Newest Version PDF]](https://arxiv.org/pdf/1711.05101.pdf)  [[Official PyTorch Implementation]](https://github.com/loshchil/AdamW-and-SGDW)  #  [[Notes]](https://www.hjhgjghhg.com/index.php/archives/172/)