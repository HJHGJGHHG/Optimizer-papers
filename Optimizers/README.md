## Optimizers
Focus on papers and related resources that propose deep learning optimization algorithms.

## Papers
* (Adam)  **Adam: A Method for Stochastic Optimization**, *Diederik P. Kingma, Jimmy Ba*, <font color="#dd0000">ICLR 2015</font>  [[PDF]](https://arxiv.org/pdf/1412.6980v9.pdf)  [[Notes]](https://www.hjhgjghhg.com/index.php/archives/151/)  [[Official PyTorch Implementation]](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam)
* (AMSGrad) **On the Convergence of Adam and Beyond**, *Sashank J. Reddi, Satyen Kale, Sanjiv Kumar*, ICLR 2018 (Best Paper)  [[ICLR PDF]](https://arxiv.org/)  [[Notes]](https://www.hjhgjghhg.com/index.php/archives/158/)  [[Official PyTorch Implementation]](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam)
* (SWATS) **Improving Generalization Performance by Switching from Adam to SGD**, *Nitish Shirish Keskar, Richard Socher*ï¼ŒarXiv 2017  [[PDF]](https://arxiv.org/pdf/1712.07628.pdf)  [[Notes]](https://www.hjhgjghhg.com/index.php/archives/166/)  [[PyTorch Implementation]](https://github.com/Mrpatekful/swats)
* (AdaBound) **Adaptive Gradient Methods with Dynamic Bound of Learning Rate**, *Liangchen Luo, Yuanhao Xiong, Yan Liu, Xu Sun*, ICLR 2019  [[ICLR PDF]](https://openreview.net/pdf?id=Bkg3g2R9FX)  [[Notes]](https://www.hjhgjghhg.com/index.php/archives/166/)  [[Official PyTorch Implementation]](https://github.com/Luolc/AdaBound)